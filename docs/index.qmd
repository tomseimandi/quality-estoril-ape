---
title: "Retraining strategies for an economic activity codification model"
author:
  - name: Thomas Faria
    email: thomas.faria@insee.fr
    affiliations:
      - ref: insee
affiliations:
  - id: insee
    name: Insee
    address: 88 avenue Verdier
    city: Montrouge
    postal-code: 92120
    country: France
bibliography: bibliography.bib
format:
  pdf:
    template-partials: 
      - _extensions/partials/title.tex
    include-in-header:
      text: |
        \usepackage[noblocks]{authblk}
        \renewcommand*{\Authsep}{, }
        \renewcommand*{\Authand}{, }
        \renewcommand*{\Authands}{, }
        \renewcommand\Affilfont{\small}
    documentclass: article
    toc: false
    number-sections: true
    link-citations: true
    colorlinks: true
    geometry:
        - top=30mm
        - left=20mm
        - heightrounded
abstract: |
  The French company registry, SIRENE, lists all companies in France and assigns them a unique identifier, the Siren number, for use by public institutions. As part of the registration process, companies must provide a description of their economic activity. Since the end of 2022, SIRENE leverages a simple text classification model to code each description into an industry from the French classification of activities (NAF).

  Using a machine learning model in a production environment comes with challenges, in particular regarding model monitoring and maintenance. In this talk, we will first present the monitoring system developed to track model behavior and detect potential drifts for SIRENE. Then we will address the question of model retraining, including the following considerations :

  - Evaluation data: how should evaluation data be collected (data quTraining data: what training data should be used for retraining ? For example, should historical data be used systematically or does the model perform better when only trained on recent data ? To what extent should data classified automaticaantity, frequency, sampling strategy) ?
  - lly by the model in production be part of the training set to keep it balanced ?
  - Retraining strategy: at what frequency ? What are the differences between fine-tuning and retraining from scratch ?
  - Algorithmic adaptations: does a more complex text classification model allow performance gains on new real-world data ?

  This talk aims to equip practitioners with an improved understanding of the technical and practical considerations involved in retraining text classification models. As such models are becoming an essential component of official statistics, it is crucial to ensure the quality of their outputs in production environments.
keywords:
  - MLOps
  - Retraining
---

# Introduction

Machine learning (ML) systems are increasingly used within national statistical institutes (NSIs) for the production of official statistics. Such systems are already in place today in several NSIs (CITATION PAPER WIESBADEN) for coding and classification of text descriptions, data editing and imputation for example. In addition to this, experiments are conducted on other natural language processing tasks, and even on computer vision systems which are very promising.

The European Statistical System has developed of common quality framework which heavily relies on the European Statistics Code of Practice (CoP). This code of practice was first adopted in 2005 and later revised in 2017. It sets standards for developing, producing and disseminating statistics, in particular by defining quality principles regarding statistical processes and output. For example, the CoP affirms that "sound methodology [and] appropriate statistical procedures, implemented throughout the statistical processes, underpin quality statistics" (principles 7 and 8). Furthermore, it states that "European Statistics [must] accurately and reliably portray reality" (principle 12). To do so, "source data, integrated data, intermediate results and statistical outputs [must be] regularly assessed and validated" and "revisions [must be] regularly analysed in order to improve source data, statistical processes and outputs".

If the ESS common quality framework has conducted European NSIs to implement quality procedures for a large part of their statistical production, there is not a lot of shared experience on what such procedures should look like for statistical outputs leveraging ML systems at some point. MLOps is a set of good practices which aims to deploy and maintain ML systems in production reliably and efficiently.

MLOps is derived from the concept of DevOps, a general software development framework the idea of which is to integrate the entire lifecycle of a project into an automated continuum. This continuity is achieved with the help of CI/CD pipelines. Continuous integration (CI) is the process of integrating code changes into a common source repository in a regular fashion. Continuous deployment (CD) is the process of automatically distributing the changes made to the source code. Consequently, the MLOps principles strive to automate the lifecycle of ML systems. Building a ML system begins with an experimental phase, where a data scientist prepares and analyses data before training a model to solve the task at hand. This model is then served to end-users through an application (typically an API). This application should be continuously monitored so as to detect potential performance losses over time. Major performance losses should trigger some form of model retraining, going back to the data preparation and analysis of the experimental phase (FIGURE).

This paper shows how MLOps principles are applied on a specific ML system at the French NSI (Insee) -- namely a codification engine for the economic activity of companies -- to ensure quality standards for downstream official statistics.

# Codification system

## Context

The French company registry, SIRENE 4, lists all companies in France and assigns them a unique identifier, the Siren number, for use by public institutions. As part of the registration process, companies must provide a description of their economic activity. Since the end of 2022, SIRENE 4 leverages a simple model to classify each text description into an industry from the French classification of activities (NAF), which contains 732 different codes. NAF activity codes are constantly used by Insee and others to produce business statistics, whenever it comes to studying companies by business sector. It is therefore paramount to ensure that the NAF codes assigned within SIRENE 4 reflect the truth. For this purpose, part of the activity descriptions are manually coded -- that is as soon as the model is not confident enough in its prediction. This will be detailed in @sec-serving.

The model currently used in a production setting (detailed in @sec-modeling) has been almost entirely trained with historical data from the former SIRENE 3 registry. Approximately 10 million observations covering the period 2014-2022 were gathered for the training procedure, with part of the ground-truth labels coming from the former rule-based codification engine Sicore, and other from manual annotation (when Sicore could not perform the codification). When evaluated on SIRENE 3 data, the model naturally exhibits a very high accuracy of 89% (along with other high performance metrics). However, SIRENE 4 data significantly differs from SIRENE 3 data (text activity descriptions are for instance longer on average), as the registration channel has been refactored in the new system. This distribution shift in the data comes with a reduced accuracy of 80% on a small evaluation set originating from the new registration channel, which has been completely hand-coded. It is important to note that this first SIRENE 4 evaluation set is biased as only certain types of companies registered through the new channel at first.

Company activities evolve over time: completely new businesses appear, businesses traditionally associated with a certain activity may see this activity evolve, etc. As a result, the accuracy of a codification engine leveraging a static model trained on a fixed training set will decrease over time. Indeed, the model would in principle not be able to correctly classify text descriptions totally unlike any description in the training set, or would tend to classify businesses associated with a changing activity according to the code mostly found in the training set.

Therefore, to maintain a highly accurate coding engine, the ML model used must be periodically retrained with an updated train set. Monitoring the activity of the coding engine will help decide when to retrain the ML model, as dicussed in @sec-monitoring. Different retraining methods are discussed in @sec-retraining.

## Modeling {#sec-modeling}

The text classification model leveraged by the SIRENE codification engine is based on a simple bag-of-ngrams model (REFERENCE), along with a standard multinomial classification head. It is implemented by the fastText library. It is an extension of the bag-of-words approach which represents a text as the average of the vector representations of each of its constituent words. In the bag-of-ngrams extension, embeddings are not only computed on words but also on word n-grams and character n-grams, providing a form of context and reducing biases due to spelling mistakes.    

In our context of supervised text classification, the embedding matrix and the classifier parameters are learned simultaneously during training by gradient descent, minimizing a cross-entropy loss function.

Additional categorical variables are in fact used to classify the text description into NAF codes. As the fastText implementation of the bag-of-ngrams model is currently used in production, a hack is used which consists in concatenating specific strings for the relevant modalities of each of the categorical variables to the text description.

## Model serving {#sec-serving}

Providing the trained model as an artifact is not a convenient way to make it available to end-users, as it assumes that they have the required knowledge to use the artifact to get predictions correctly, which often requires specific computation environment, etc. Serving a model this way always leads to code duplication and very often to errors. In an effort to build an interoperable and easy-to-use codification engine, available for query from various programming languages, our text classification model is served through a REST API. REST APIs have become a standard way to serve ML models. Among theirs advantages, they rely on standards technologies for queries (HTTP requests) and responses (generally, JSON-formatted strings), making them agnostic to the programming language used to query them, and they offer great scalability because of their stateless design.

The REST API is developed using FastAPI, a modern, high-performance, web framework for building APIs with Python. We encapsulate the API code and its dependencies into a Docker image, which is deployed as a container on the Kubernetes cluster. This grants the ability to easily scale the API according to demand thanks to automatic load-balancing. Uponstartup, the API automatically retrieves the text classification model from storage. The model is packaged as an MLflow model, which integrates pre-processing steps in its inference method. This means that clients do not have to apply these steps themselves, which prevents many potential mistakes.  

# Monitoring {#sec-monitoring}

## Design

## Engine monitoring

## Continuous performance evaluation

# Retraining {#sec-retraining}

## Triggers

## Retraining strategies

# Discussion

# References
