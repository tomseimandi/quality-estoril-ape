@article{gjaltema2022,
  title={High-Level Group for the Modernisation of Official Statistics (HLG-MOS) of the United Nations Economic Commission for Europe},
  author={Gjaltema, Taeke},
  journal={Statistical Journal of the IAOS},
  volume={38},
  number={3},
  pages={917--922},
  year={2022},
  publisher={IOS Press}
}

@book{cop2018,
  place = {LU},
  title = {European statistics code of practice: for the national statistical authorities and Eurostat (EU statistical authority).},
  url = {https://data.europa.eu/doi/10.2785/798269},
  DOI = {10.2785/798269},
  publisher = {Publications Office},
  author = {Statistical Office of the European Union},
  year = {2018}
}

@article{joulin2016,
  author       = {Armand Joulin and
                  Edouard Grave and
                  Piotr Bojanowski and
                  Tom{\'{a}}s Mikolov},
  title        = {Bag of Tricks for Efficient Text Classification},
  journal      = {CoRR},
  volume       = {abs/1607.01759},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.01759},
  eprinttype    = {arXiv},
  eprint       = {1607.01759},
  timestamp    = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/JoulinGBM16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{martin-etal-2020-camembert,
    title = "{C}amem{BERT}: a Tasty {F}rench Language Model",
    author = "Martin, Louis  and
      Muller, Benjamin  and
      Ortiz Su{\'a}rez, Pedro Javier  and
      Dupont, Yoann  and
      Romary, Laurent  and
      de la Clergerie, {\'E}ric  and
      Seddah, Djam{\'e}  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.645",
    pages = "7203--7219",
    abstract = "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",
}